{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "japanese_word2vec.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomaOkuda/Tweet-Analysis/blob/main/japanese_word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okdJcn--ngzf"
      },
      "source": [
        "!pip install janome\n",
        "#MeCabインストール\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.996.3\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a\n",
        "!pip install emoji\n",
        "!pip install japanize-matplotlib\n",
        "!pip install gensim==3.4.0\n",
        "!pip install smart_open==1.9.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN3NzfPXntVt"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubDw7QmW2Vw6"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import smart_open\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "import glob\n",
        "import pickle\n",
        "import emoji\n",
        "\n",
        "\n",
        "import MeCab\n",
        "tagger=MeCab.Tagger('-Owakati')\n",
        "\n",
        "from janome.tokenizer import Tokenizer\n",
        "import time\n",
        "# Tokenizerインスタンスの生成 \n",
        "t = Tokenizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLMMhCTwjn_D"
      },
      "source": [
        "csv_files1 = glob.glob('/content/drive/My Drive/Tweet/tweet/auto_test/*.csv') #jpサンプルツイートフォルダ内結合\n",
        "data_list1 = []\n",
        "for file in csv_files1:\n",
        "  data_list1.append(pd.read_csv(file, lineterminator='\\n'))  \n",
        "df = pd.concat(data_list1, axis=0, sort=False)\n",
        "df = df.drop_duplicates(keep='last') #重複の削除、列の最後の方が時刻が遅いため、lastで指定\n",
        "df=df.reset_index(drop=True)\n",
        "\n",
        "elim=[]\n",
        "for i in range(len(df)):\n",
        "  if str(df.iloc[i]['text']).startswith('RT') is True:\n",
        "    elim.append(i)\n",
        "\n",
        "#RT除去\n",
        "df1=df.drop(elim)\n",
        "df1=df1.reset_index(drop=True)\n",
        "len(df1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-uCj5jzG7On"
      },
      "source": [
        "elim_http=[]\n",
        "for j in range(len(df1)):\n",
        "  user_text=df1.iloc[j]['text']\n",
        "  word_list=list(tagger.parse(user_text).split())\n",
        "  if 'https' in word_list:\n",
        "    elim_http.append(j)\n",
        "  elif 'http' in word_list:\n",
        "    elim_http.append(j)\n",
        "  elif '@' in word_list:\n",
        "    elim_http.append(j)\n",
        "\n",
        "#http除去\n",
        "df2=df1.drop(elim_http)\n",
        "df2=df2.reset_index(drop=True)\n",
        "len(df2) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM5BxRf_GzWn"
      },
      "source": [
        "def Remove_word(list):\n",
        "  rems = ['@', ',', '，', '。', '', ' ', 'http', 'https','//', '、', '×', '.',\n",
        "            '+', '＋', '/',', ', '(', ')', '（', '）', '://', 'RT', ':', ';','：', '；',\n",
        "            '」','「', '＠', '-', '／',  '%', '％', '!', '！', '#', '＃',\n",
        "            '$', '＄', '=', '＝', 'ー', '￥', '^', '{', '}', '~', '～', \n",
        "            '&', '＆', '・', '?', '??', '？', '？？',  '<', '＜', '>', '＞', '_', '＿', \n",
        "            'co', 'jp', '(@', '○', '『', '』', '”', '’', '\"', '…', '‼', '(+'\n",
        "            ')」', '〇', '【', '】', '[', ']', '⁉', 'gt',\n",
        "            'から', 'より', 'こそ', 'でも', 'しか','さえ', 'けれど', 'たり', 'つつ', 'とも',\n",
        "            'たら', 'ある', 'なら', 'のに', 'です', 'ます', 'する', 'ほど', 'ない', 'くる',\n",
        "            'なり', 'そう', 'まし', 'その', 'この', 'あの', 'せる', 'どう', 'ため', 'どこ',\n",
        "            'いる', 'これ', 'それ', 'あれ', 'いい', 'など', 'あっ', 'もう', 'さん', 'じゃ',\n",
        "            'から', 'あり', 'ので', 'とも', 'ませ', 'でし', 'とき', 'こと', 'なる', 'って',\n",
        "            'ただ', 'まで', 'もの', 'つか', 'なっ', 'でき', 'もっ', 'けど', 'ほぼ', 'なー',\n",
        "            'そこ', 'ここ', 'だろ', 'なん', 'だっ', 'なあ', 'っけ', 'せる', 'やっ', 'また',\n",
        "            'どれ', 'なれ', 'かも', 'いく', 'いけ', 'いう', 'たい', 'あと', 'かも', 'しれ',  \n",
        "            'こう', 'なく', 'よく', 'だけ', 'れる', 'よう', 'かけ', 'どの', 'てる', 'とか',\n",
        "            'という', '思う', '思っ', 'として', 'ところ', 'しまう', 'なんか', 'そんな', \n",
        "            'でしょ', 'しまい', 'わかり', 'まま'\n",
        "            '\\u200d', '\\u3000#'\n",
        "            ]\n",
        "  for number in range(10000):\n",
        "    rems.append(str(number))\n",
        "\n",
        "# 一文字のひらがなとカタカナ，全角数字，アルファベット大文字・小文字\n",
        "  hirakana    = [chr(i) for i in range(12353, 12436)]\n",
        "  katakana    = [chr(i) for i in range(12449, 12533)]\n",
        "  zenkaku_num = [chr(i) for i in range(65296, 65296+10)]\n",
        "  alph_L      = [chr(i) for i in range(97, 97+26)]\n",
        "  alph_S      = [chr(i) for i in range(65, 65+26)]\n",
        "\n",
        "  for a in range(len(hirakana)):\n",
        "    rems.append(hirakana[a])\n",
        "  for b in range(len(katakana)):\n",
        "    rems.append(katakana[b])\n",
        "  for c in range(len(zenkaku_num)):\n",
        "    rems.append(zenkaku_num[c])\n",
        "  for d in range(len(alph_L)):\n",
        "    rems.append(alph_L[d])\n",
        "  for e in range(len(alph_S)):\n",
        "    rems.append(alph_S[e])\n",
        "\n",
        "  words = [x for x in list if (x not in rems)]\n",
        "  return words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd15fH1LjRZl"
      },
      "source": [
        "# テキストを引数として、形態素解析の結果、名詞・動詞・形容詞(原形)のみを配列で抽出する関数を定義 \n",
        "def extract_word(text):\n",
        "    tokens = t.tokenize(text)\n",
        "    return [token.base_form for token in tokens \n",
        "        if token.part_of_speech.split(',')[0] in['名詞', '動詞','形容詞']]\n",
        "\n",
        "\n",
        "all_users_words=[]\n",
        "df3=df2.sample(frac=0.5) #ツイート数が大きいから50%だけ抽出してる\n",
        "df3=df3.reset_index(drop=True)\n",
        "\n",
        "for i in range(len(df3)):\n",
        "  user_text=df3.iloc[i]['text']\n",
        "  #text=list(tagger.parse(user_text).split())\n",
        "  words=extract_word(user_text)\n",
        "  all_users_words.append(words)\n",
        "  \n",
        "all_users_words1=[e for inner_list in all_users_words for e in inner_list]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKDiQ4IpcGJK"
      },
      "source": [
        "import pickle\n",
        "f = open('/content/drive/My Drive/Tweet/w2b/210110_vaccine.txt', 'wb')\n",
        "list_row = all_users_words1\n",
        "pickle.dump(list_row, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpsFJBiy3Et7"
      },
      "source": [
        "BoW構築"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ffE0Cy-kL8O"
      },
      "source": [
        "def id_preprocess(words, word_to_id, id_to_word):\n",
        "    for word in words:\n",
        "        if word not in word_to_id:\n",
        "            new_id = len(word_to_id)\n",
        "            word_to_id[word] = new_id\n",
        "            id_to_word[new_id] = word\n",
        "    return word_to_id, id_to_word\n",
        "\n",
        "def corpus_preprocess(words, word_to_id, id_to_word):\n",
        "    corpus = np.array([word_to_id[w] for w in words])\n",
        "    return corpus\n",
        "\n",
        "word_to_id = {}\n",
        "id_to_word = {}\n",
        "corpus_dic = {}\n",
        "\n",
        "word_to_id, id_to_word = id_preprocess(all_users_words1, word_to_id, id_to_word)\n",
        "corpus_dic = corpus_preprocess(all_users_words1, word_to_id, id_to_word)\n",
        "\n",
        "bag_of_words = {}\n",
        "vocab_size = len(word_to_id) # = len(id_to_word) \n",
        "\n",
        "bag_of_words = {}\n",
        "corpus_size = len(corpus_dic)\n",
        "  \n",
        "for i in range(vocab_size):\n",
        "  count = 0\n",
        "  for j in range(corpus_size):\n",
        "    try:\n",
        "      if corpus_dic[j] == i: # 元の文書を走査していき，一致していればカウントする。\n",
        "        count += 1\n",
        "      else:\n",
        "        pass\n",
        "    except:\n",
        "      pass\n",
        "  bag_of_words[id_to_word[i]] = count\n",
        "  \n",
        "bag_of_words = sorted(bag_of_words.items(), key = lambda x: x[1], reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRkUraJFw45H"
      },
      "source": [
        "df_bow = pd.DataFrame(bag_of_words)\n",
        "df_bow.to_csv('/content/drive/MyDrive/Tweet/w2b/220110_including_vaccine.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5b45hs-24SQ"
      },
      "source": [
        "*Twitterワクチンを含むツイートのWord2vec構築*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JDlrG4F7TOe"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "Table=pd.read_csv('/content/drive/MyDrive/Tweet/w2b/220110_including_vaccine.csv')\n",
        "word_df=Table[Table[\"1\"]>=5]\n",
        "word_df=word_df.reset_index(drop=True)\n",
        "len(word_df)\n",
        "#words_exist=word_df[\"0\"].values.tolist()\n",
        "#print(words_exist[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79_z0-XcnmE7"
      },
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "import time\n",
        "\n",
        "# Tokenizerインスタンスの生成 \n",
        "t = Tokenizer()\n",
        "\n",
        "# テキストを引数として、形態素解析の結果、名詞・動詞・形容詞(原形)のみを配列で抽出する関数を定義 \n",
        "def extract_words(text):\n",
        "    tokens = t.tokenize(text)\n",
        "    return [token.base_form for token in tokens \n",
        "        if token.part_of_speech.split(',')[0] in['名詞', '動詞', '形容詞']]\n",
        "\n",
        "# 全体のテキストを句点('。')で区切った配列にする。 \n",
        "sent = list(df3[\"text\"])\n",
        "# それぞれの文章を単語リストに変換(処理に数分かかります)\n",
        "\n",
        "time1=time.time()\n",
        "word_list=[]\n",
        "for sentence in sent:\n",
        "  word_list.append(extract_words(sentence))\n",
        "\n",
        "time2=time.time()\n",
        "total_time=time2-time1\n",
        "print(total_time)\n",
        "#estimate:40分"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KhQF0VtdP9N-"
      },
      "source": [
        "model = word2vec.Word2Vec(word_list,\n",
        "                          size=50,  # ベクトルの次元\n",
        "                          min_count=3,  # 最小3回以上出ている単語のみの使用\n",
        "                          window=5,  # window幅\n",
        "                          iter=100  # 学習の繰り返し数\n",
        "                         )\n",
        "\n",
        "model.save(\"testmodel\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3rVy5ugvUMN"
      },
      "source": [
        "cos類似度探索＊単語の類似度を測る"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jkLmjbXQgZE"
      },
      "source": [
        "#model= word2vec.Word2Vec.load(\"testmodel_1\")\n",
        "ret = model.wv.most_similar(positive=['XX']) \n",
        "for item in ret:\n",
        "    print(item[0], item[1])\n",
        "#絵文字含まれない"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPzH2nJMYiat"
      },
      "source": [
        "def cal_cos_similarity(q, d):\n",
        "  return np.dot(q, d) / (np.linalg.norm(q) * np.linalg.norm(d))\n",
        "\n",
        "print(cal_cos_similarity(model.wv['XX'], model.wv['XX']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFDVZDjCIC3Q"
      },
      "source": [
        "total_cos_list=[]\n",
        "words_exist=word_df[\"0\"].values.tolist()\n",
        "for word in words_exist:\n",
        "  cos_list=[]\n",
        "  for word2 in words_exist:\n",
        "    cos_list.append(cal_cos_similarity(model.wv[word], model.wv[word2]))\n",
        "  total_cos_list.append(cos_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq-7CkctxFbj"
      },
      "source": [
        "df_cos_similarity=pd.DataFrame({'name': words_exist,\n",
        "                                'similarity': total_cos_list})\n",
        "\n",
        "splitted = df_cos_similarity['similarity'].apply(pd.Series)\n",
        "splitted.columns = words_exist\n",
        "df_cos_similarity=pd.concat([df_cos_similarity['name'], splitted], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxT2c-HexKOe"
      },
      "source": [
        "df_cos_similarity.to_csv('cos_similarity.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u2IMpxEklsJF"
      },
      "source": [
        "print(df_cos_similarity)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlXHsLtpxfJ9"
      },
      "source": [
        "#df1=df.drop('Unnamed: 0', axis=1)\n",
        "df_cos_similarity1=df_cos_similarity.set_index('name')\n",
        "#df2=df1.sample(frac=1)\n",
        "print(df_cos_similarity1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sCYfoRRbl7Mh"
      },
      "source": [
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import japanize_matplotlib #日本語化matplotlib\n",
        "import seaborn as sns\n",
        "sns.set(font=\"IPAexGothic\") #日本語フォント設定\n",
        "\n",
        "result1 = linkage(df_cos_similarity1, \n",
        "                  metric = 'braycurtis', \n",
        "                  #metric = 'canberra', \n",
        "                  #metric = 'chebyshev', \n",
        "                  #metric = 'cityblock', \n",
        "                  #metric = 'correlation', \n",
        "                  #metric = 'cosine', \n",
        "                  #metric = 'euclidean', \n",
        "                  #metric = 'hamming', \n",
        "                  #metric = 'jaccard', \n",
        "                  #method= 'single')\n",
        "                  method = 'average')\n",
        "                  #method= 'complete')\n",
        "                  #method='weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wy-GcHvmezf"
      },
      "source": [
        "# 指定したクラスタ数でクラスタを得る関数を作る。\n",
        "def get_cluster_by_number(result, number):\n",
        "    output_clusters = []\n",
        "    x_result, y_result = result.shape\n",
        "    n_clusters = x_result + 1\n",
        "    cluster_id = x_result + 1\n",
        "    father_of = {}\n",
        "    df1 = pd.DataFrame(result)\n",
        "    x1 = []\n",
        "    y1 = []\n",
        "    x2 = []\n",
        "    y2 = []\n",
        "    for i in df1.index:\n",
        "        n1 = int(df1.iloc[i][0])\n",
        "        n2 = int(df1.iloc[i][1])\n",
        "        val = df1.iloc[i][2]\n",
        "        n_clusters -= 1\n",
        "        if n_clusters >= number:\n",
        "            father_of[n1] = cluster_id\n",
        "            father_of[n2] = cluster_id\n",
        "\n",
        "        cluster_id += 1\n",
        "\n",
        "    cluster_dict = {}\n",
        "    for n in range(x_result + 1):\n",
        "        if not n in father_of:\n",
        "            output_clusters.append([n])\n",
        "            continue\n",
        "\n",
        "        n2 = n\n",
        "        m = False\n",
        "        while n2 in father_of:\n",
        "            m = father_of[n2]\n",
        "            #print [n2, m]\n",
        "            n2 = m\n",
        "\n",
        "        if not m in cluster_dict:\n",
        "            cluster_dict.update({m:[]})\n",
        "        cluster_dict[m].append(n)\n",
        "\n",
        "    output_clusters += cluster_dict.values()\n",
        "\n",
        "    output_cluster_id = 0\n",
        "    output_cluster_ids = [0] * (x_result + 1)\n",
        "    for cluster in sorted(output_clusters):\n",
        "        for i in cluster:\n",
        "            output_cluster_ids[i] = output_cluster_id\n",
        "        output_cluster_id += 1\n",
        "\n",
        "    return output_cluster_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ov1L4vzwmvQX"
      },
      "source": [
        "cluster_n=50\n",
        "cluster=get_cluster_by_number(result1, cluster_n)\n",
        "df_cluster=pd.DataFrame({'name': df_cos_similarity.name, \n",
        "                         'cluster': cluster})\n",
        "df_cluster_dict=df_cluster.set_index('name').to_dict(orient='dict')\n",
        "df_cluster_dict=df_cluster_dict['cluster']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9xZOmUrnVLi"
      },
      "source": [
        "word_list = []\n",
        "import csv\n",
        "for i in range(50):\n",
        "  df_i=df_cluster[df_cluster['cluster']==i]\n",
        "  print(i)\n",
        "  print(list(df_i.name)[:20])\n",
        "  #word_list.append(list(df_i.name)[:50])\n",
        "#with open('/content/drive/My Drive/Tweet/w2b/vaccine.csv', 'w') as f:\n",
        "     # write = csv.writer(f) \n",
        "    #  write.writerow(\"words\")\n",
        "   #   write.writerows(word_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOV8wEPtnDYl"
      },
      "source": [
        "f = open(\"/content/drive/My Drive/Tweet/w2b/vaccine.txt\",\"rb\")\n",
        "all_users_BoW = pickle.load(f)\n",
        "\n",
        "all_freq=[]\n",
        "for bow in all_users_BoW:\n",
        "  Table = pd.DataFrame(bow)\n",
        "  cluster_n=[]\n",
        "  for i in range(len(Table)):\n",
        "    if Table.iloc[i][0] in df_cluster_dict:\n",
        "      cluster_n.append(df_cluster_dict[Table.iloc[i][0]])\n",
        "    else:\n",
        "      cluster_n.append('error')\n",
        "  Table['cluster_n']=pd.Series(cluster_n)\n",
        "\n",
        "  freq=[]\n",
        "  freq1=[]\n",
        "  for i in range(200):\n",
        "    df_c=Table[Table['cluster_n']==i]\n",
        "    df_c=df_c.reset_index(drop=True)\n",
        "    freq.append(df_c[1].sum())\n",
        "  for i in range(len(freq)):\n",
        "    freq1.append(freq[i]*100/sum(freq))\n",
        "  all_freq.append(freq1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GNOVwE1lnYv3"
      },
      "source": [
        "df_pred=pd.read_excel('/content/drive/MyDrive/Income_prediction/0308/0317_cluster_default.xlsx')\n",
        "c=pd.DataFrame(all_freq)\n",
        "df_pred=pd.concat([df_pred, c], axis=1)\n",
        "df_pred.to_excel('/content/drive/MyDrive/Income_prediction/0324_jp_twitter_w2v/0406_cluster_200.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRTU9nUznnyP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}