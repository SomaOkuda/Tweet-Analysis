{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SomaOkuda/Tweet-Analysis/blob/main/emotion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hmz7g5g3Sit9"
      },
      "outputs": [],
      "source": [
        "!pip install pymlask\n",
        "!pip install janome\n",
        "#MeCabインストール\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3==0.996.3\n",
        "!git clone --depth 1 https://github.com/neologd/mecab-ipadic-neologd.git\n",
        "!echo yes | mecab-ipadic-neologd/bin/install-mecab-ipadic-neologd -n -a\n",
        "!pip install emoji\n",
        "!pip install japanize-matplotlib\n",
        "!pip install gensim==3.4.0\n",
        "!pip install smart_open==1.9.0\n",
        "!pip install WordCroud\n",
        "!apt-get -y install fonts-ipafont-gothic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uo-ZDRE3YqbZ",
        "outputId": "083b0c6c-b16e-46f2-a3da-af29d25c4a61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eapZldDbY0Sw"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import smart_open\n",
        "import gensim\n",
        "from gensim.models import word2vec\n",
        "import glob\n",
        "import pickle\n",
        "import emoji\n",
        "import csv\n",
        "import re\n",
        "from mlask import MLAsk\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import math\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "emotion_analyzer = MLAsk()\n",
        "\n",
        "import MeCab\n",
        "tagger=MeCab.Tagger('-Owakati')\n",
        "\n",
        "from janome.tokenizer import Tokenizer\n",
        "import time\n",
        "# Tokenizerインスタンスの生成 \n",
        "t = Tokenizer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYIbrClzaFL5"
      },
      "outputs": [],
      "source": [
        "csv_files = glob.glob('/content/drive/My Drive/Tweet/tweet/auto_test/*.csv') #jpサンプルツイートフォルダ内結合\n",
        "data_list = []\n",
        "for file in csv_files:\n",
        "  data_list.append(pd.read_csv(file, lineterminator='\\n'))  \n",
        "df = pd.concat(data_list, axis=0, sort=False)\n",
        "df = df.drop_duplicates(keep='last') #重複の削除、列の最後の方が時刻が遅いため、lastで指定\n",
        "df=df.reset_index(drop=True)\n",
        "\n",
        "elim=[]\n",
        "for i in range(len(df)):\n",
        "  if str(df.iloc[i]['text']).startswith('RT') is True:\n",
        "    elim.append(i)\n",
        "\n",
        "#RT除去\n",
        "df1=df.drop(elim)\n",
        "df1=df1.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eu5dvboCaNaa"
      },
      "outputs": [],
      "source": [
        "elim_http=[]\n",
        "for j in range(len(df1)):\n",
        "  user_text=df1.iloc[j]['text']\n",
        "  word_list=list(tagger.parse(user_text).split())\n",
        "  if 'https' in word_list:\n",
        "    elim_http.append(j)\n",
        "  elif 'http' in word_list:\n",
        "    elim_http.append(j)\n",
        "  elif '@' in word_list:\n",
        "    elim_http.append(j)\n",
        "\n",
        "#http除去\n",
        "df2=df1.drop(elim_http)\n",
        "df2=df2.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcClz7gJaR2p"
      },
      "outputs": [],
      "source": [
        "emotion_list = []\n",
        "orientation_list = []\n",
        "\n",
        "for k in range(len(df2)):\n",
        "  text = df2.iloc[k]['text']\n",
        "  result = emotion_analyzer.analyze(text)\n",
        "  try:\n",
        "    orientation = result['orientation']\n",
        "    kanjo = result['representative']\n",
        "    emotion_list.append(kanjo)\n",
        "    orientation_list.append(orientation)\n",
        "  except KeyError:\n",
        "    kanjo = 'NA'\n",
        "    orientation = 'NA'\n",
        "    emotion_list.append(kanjo)\n",
        "    orientation_list.append(orientation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Lucl61005v8"
      },
      "outputs": [],
      "source": [
        "df2[\"orientation\"] = orientation_list\n",
        "df2[\"emotion\"] = emotion_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xI7QxZjL5BmL"
      },
      "outputs": [],
      "source": [
        "orientation_negative = (df2['orientation'] == 'NEGATIVE')\n",
        "orientation_positive = (df2['orientation'] == 'POSITIVE')\n",
        "orientation_neutral = (df2['orientation'] == 'NEUTRAL')\n",
        "orientation_mone = (df2['orientation'] == 'mostly_NEGATIVE')\n",
        "orientation_mopo = (df2['orientation'] == 'mostly_POSITIVE')\n",
        "orientation_na = (df2['orientation'] == 'NA')\n",
        "#RTやhttpを除くツイートの感情(positive vs. negative)の図示\n",
        "objects_orientation = ('Negative', 'Mostly Negative', 'Neutral' ,'Mostly Positive', 'Positive', 'NA')\n",
        "y_pos = np.arange(len(objects_orientation))\n",
        "performance = [orientation_negative.sum(), orientation_mone.sum(), orientation_neutral.sum(), orientation_mopo.sum(), orientation_positive.sum(), orientation_na.sum()]\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(y_pos, performance, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_orientation)\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.title('Tweet Emotion Orientation')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8iRy_5_A7r5X"
      },
      "outputs": [],
      "source": [
        "#Russelの感情円環に基づいて10感情が分類される\n",
        "#喜, 怒, 哀, 怖, 恥, 好, 厭, 昂, 安, 驚\n",
        "objects_emotion = ('joy', 'anger', 'sorrow', 'fear', 'shame', 'liking', 'dislike', 'excitement', 'relief', 'surprise', 'NA')\n",
        "df3 = df2['emotion']\n",
        "df4 = []\n",
        "for i in range(len(df3)):\n",
        "  row = str(df3[i])\n",
        "  df4.append(row)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y-hYj7rgCiP"
      },
      "outputs": [],
      "source": [
        "emotion_yorokobi = [s for s in df4 if 'yorokobi' in s]\n",
        "emotion_ikari = [s for s in df4 if 'ikari' in s]\n",
        "emotion_aware = [s for s in df4 if 'aware' in s]\n",
        "emotion_kowagari = [s for s in df4 if 'kowa' in s]\n",
        "emotion_haji = [s for s in df4 if 'haji' in s]\n",
        "emotion_suki = [s for s in df4 if 'suki' in s]\n",
        "emotion_iya = [s for s in df4 if 'iya' in s]\n",
        "emotion_takaburi = [s for s in df4 if 'takaburi' in s]\n",
        "emotion_yasuragi = [s for s in df4 if 'yasu' in s]\n",
        "emotion_odoroki = [s for s in df4 if 'odoroki' in s]\n",
        "emotion_na = [s for s in df4 if 'NA' in s]\n",
        "\n",
        "y_pos = np.arange(len(objects_emotion))\n",
        "performance = [len(emotion_yorokobi), len(emotion_ikari), len(emotion_aware), len(emotion_kowagari), len(emotion_haji), len(emotion_suki), len(emotion_iya), len(emotion_takaburi), len(emotion_yasuragi), len(emotion_odoroki), len(emotion_na)]\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.bar(y_pos, performance, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_emotion)\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.title('Tweet Emotion')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjdVtZgaCWdJ"
      },
      "source": [
        "感情とRT数の分析。cytoscapeで抽出したRT数（degree.layout）と感情の関係を調べる"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NeEOICQlrkK_"
      },
      "outputs": [],
      "source": [
        "#感情とRT数\n",
        "#ネットワークdegree.laをすることにより、リンクの数がわかる。このリンクの数と感情の関係を示す（今後はfollowerも必要）\n",
        "df99 = pd.read_csv(\"/content/drive/My Drive/Tweet/RT_Emotion/220110_vactine_propagation_RT_dgt2.gml default node.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9RwszFhWvbb"
      },
      "outputs": [],
      "source": [
        "df99 = df99.fillna(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSDk338eATfi"
      },
      "outputs": [],
      "source": [
        "#リンク数が少ないものを除去する（多くの人に共有されるツイートはどういうツイートが多いのか？）\n",
        "elim99=[]\n",
        "for i in range(len(df99)):\n",
        "  if df99[\"degree.layout\"][i] < 30.0: #30以上のRTがあるオリジナルツイートのみを摘出\n",
        "    elim99.append(i)\n",
        "\n",
        "df100=df99.drop(elim99)\n",
        "df100=df100.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PU-VJXTdBH2k"
      },
      "outputs": [],
      "source": [
        "emo_rt = []\n",
        "for j in range(len(df100)):\n",
        "  for k in range(len(df2)):\n",
        "    if df100[\"name\"][j] == df2[\"user_name\"][k]:\n",
        "      emo = str(df2[\"emotion\"][k])\n",
        "      newline = [df100[\"degree.layout\"][j], df2[\"orientation\"][k], emo, df100[\"cluster\"][j], df2[\"follower\"][k]]\n",
        "      emo_rt.append(newline)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbyKBOr3D-xf"
      },
      "outputs": [],
      "source": [
        "emo_rt = pd.DataFrame(emo_rt, columns=[\"degree.layout\", \"orientation\", \"emotion\", \"cluster\", \"follower\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEIg2x4KTVwQ"
      },
      "outputs": [],
      "source": [
        "emotion_yorokobi_gt50 = [s for s in emo_rt[\"emotion\"] if 'yorokobi' in s]\n",
        "emotion_ikari_gt50 = [s for s in emo_rt[\"emotion\"] if 'ikari' in s]\n",
        "emotion_aware_gt50 = [s for s in emo_rt[\"emotion\"] if 'aware' in s]\n",
        "emotion_kowagari_gt50 = [s for s in emo_rt[\"emotion\"] if 'kowa' in s]\n",
        "emotion_haji_gt50 = [s for s in emo_rt[\"emotion\"] if 'haji' in s]\n",
        "emotion_suki_gt50 = [s for s in emo_rt[\"emotion\"] if 'suki' in s]\n",
        "emotion_iya_gt50 = [s for s in emo_rt[\"emotion\"] if 'iya' in s]\n",
        "emotion_takaburi_gt50 = [s for s in emo_rt[\"emotion\"] if 'takaburi' in s]\n",
        "emotion_yasuragi_gt50 = [s for s in emo_rt[\"emotion\"] if 'yasu' in s]\n",
        "emotion_odoroki_gt50 = [s for s in emo_rt[\"emotion\"] if 'odoroki' in s]\n",
        "emotion_na_gt50 = [s for s in emo_rt[\"emotion\"] if 'NA' in s]\n",
        "\n",
        "y_pos = np.arange(len(objects_emotion))\n",
        "performance_emo_gt50 = [len(emotion_yorokobi_gt50), len(emotion_ikari_gt50), len(emotion_aware_gt50), len(emotion_kowagari_gt50), len(emotion_haji_gt50), len(emotion_suki_gt50), len(emotion_iya_gt50), len(emotion_takaburi_gt50), len(emotion_yasuragi_gt50), len(emotion_odoroki_gt50), len(emotion_na_gt50)]\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.bar(y_pos, performance_emo_gt50, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_emotion)\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.title('Emotion of Original Tweet *greater than 30')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Dqhh7GYt8b0"
      },
      "outputs": [],
      "source": [
        "orientation_negative_gt50 = (emo_rt[\"orientation\"] == 'NEGATIVE')\n",
        "orientation_positive_gt50 = (emo_rt[\"orientation\"] == 'POSITIVE')\n",
        "orientation_neutral_gt50 = (emo_rt[\"orientation\"] == 'NEUTRAL')\n",
        "orientation_mone_gt50 = (emo_rt[\"orientation\"] == 'mostly_NEGATIVE')\n",
        "orientation_mopo_gt50 = (emo_rt[\"orientation\"] == 'mostly_POSITIVE')\n",
        "orientation_na_gt50 = (emo_rt[\"orientation\"] == 'NA')\n",
        "objects_orientation = ('Negative', 'Mostly Negative', 'Neutral' ,'Mostly Positive', 'Positive', 'NA')\n",
        "y_pos = np.arange(len(objects_orientation))\n",
        "performance_ori_gt50 = [orientation_negative_gt50.sum(), orientation_mone_gt50.sum(), orientation_neutral_gt50.sum(), orientation_mopo_gt50.sum(), orientation_positive_gt50.sum(), orientation_na_gt50.sum()]\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(y_pos, performance_ori_gt50, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_orientation)\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.title('Tweet Emotion Orientation greater than 30')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FObUgGa7ACnn"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(x=\"follower\", y=\"degree.layout\", data=emo_rt, ci=95, hue=\"cluster\")\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "#plt.xlabel('followers')\n",
        "#plt.ylabel('Number of Retweets')\n",
        "plt.title('ReTweet Number and followers *greater than 30')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gv8TCCHgI4Vd"
      },
      "outputs": [],
      "source": [
        "sns.regplot(x=\"follower\", y=\"degree.layout\", data=emo_rt, ci=95, fit_reg=True)\n",
        "plt.xscale('log')\n",
        "plt.yscale('log')\n",
        "plt.title('ReTweet Number and followers *greater than 30')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vVlXvWGM_ir"
      },
      "outputs": [],
      "source": [
        "x = emo_rt[\"follower\"]\n",
        "y = emo_rt[\"degree.layout\"]\n",
        "coef1 = np.corrcoef(x, y)\n",
        "#coef2 = np.corrcoef(math.log10(x), math.log10(y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pxxghKa3b0w4"
      },
      "outputs": [],
      "source": [
        "num_yorokobi_gt50 = 0\n",
        "num_ikari_gt50 = 0\n",
        "num_aware_gt50 = 0\n",
        "num_kowagari_gt50 = 0\n",
        "num_haji_gt50 = 0\n",
        "num_suki_gt50 = 0\n",
        "num_iya_gt50 = 0\n",
        "num_takaburi_gt50 = 0\n",
        "num_yasuragi_gt50 = 0\n",
        "num_odoroki_gt50 = 0\n",
        "num_NA_gt50 = 0\n",
        "\n",
        "for s in range(len(emo_rt)):\n",
        "  if 'yorokobi' in emo_rt[\"emotion\"][s]:\n",
        "    num_yorokobi_gt50 = num_yorokobi_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'ikari' in emo_rt[\"emotion\"][s]:\n",
        "    num_ikari_gt50 = num_ikari_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'aware' in emo_rt[\"emotion\"][s]:\n",
        "    num_aware_gt50 =  num_aware_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'kowa' in emo_rt[\"emotion\"][s]:\n",
        "    num_kowagari_gt50 = num_kowagari_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'haji' in emo_rt[\"emotion\"][s]:\n",
        "    num_haji_gt50 = num_haji_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'suki' in emo_rt[\"emotion\"][s]:\n",
        "    num_suki_gt50 = num_suki_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'iya' in emo_rt[\"emotion\"][s]:\n",
        "    num_iya_gt50 = num_iya_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'takaburi' in emo_rt[\"emotion\"][s]:\n",
        "    num_takaburi_gt50 = num_takaburi_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'yasu' in emo_rt[\"emotion\"][s]:\n",
        "    num_yasuragi_gt50 = num_yasuragi_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'odoroki' in emo_rt[\"emotion\"][s]:\n",
        "    num_odoroki_gt50 = num_odoroki_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'NA' in emo_rt[\"emotion\"][s]:\n",
        "    num_NA_gt50 = num_NA_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "\n",
        "y_pos = np.arange(len(objects_emotion))\n",
        "performance_gt50_num = [num_yorokobi_gt50, num_ikari_gt50, num_aware_gt50, num_kowagari_gt50, num_haji_gt50, num_suki_gt50, num_iya_gt50, num_takaburi_gt50, num_yasuragi_gt50, num_odoroki_gt50, num_NA_gt50]\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.bar(y_pos, performance_gt50_num, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_emotion)\n",
        "plt.ylabel('Number of Retweets')\n",
        "plt.title('ReTweet Number and Emotion *greater than 30')\n",
        "print(performance_gt50_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o-eoRJ1hwXfv"
      },
      "outputs": [],
      "source": [
        "num_n_gt50 = 0\n",
        "num_mn_gt50 = 0\n",
        "num_nu_gt50 = 0\n",
        "num_mp_gt50 = 0\n",
        "num_p_gt50 = 0\n",
        "num_na_gt50 = 0\n",
        "\n",
        "for s in range(len(emo_rt)):\n",
        "  if 'NEGATIVE' == emo_rt[\"orientation\"][s]:\n",
        "    num_n_gt50 = num_n_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'mostly_NEGATIVE' == emo_rt[\"orientation\"][s]:\n",
        "    num_mn_gt50 = num_mn_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'NEUTRAL' in emo_rt[\"orientation\"][s]:\n",
        "    num_nu_gt50 =  num_nu_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'mostly_POSITIVE' == emo_rt[\"orientation\"][s]:\n",
        "    num_mp_gt50 = num_mp_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'POSITIVE' == emo_rt[\"orientation\"][s]:\n",
        "    num_p_gt50 = num_p_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "  elif 'NA' == emo_rt[\"orientation\"][s]:\n",
        "    num_na_gt50 = num_na_gt50 + emo_rt[\"degree.layout\"][s]\n",
        "\n",
        "y_pos = np.arange(len(objects_orientation))\n",
        "performance_ori_gt50 = [num_n_gt50, num_mn_gt50, num_nu_gt50, num_mp_gt50, num_p_gt50, num_na_gt50]\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(y_pos, performance_ori_gt50, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_orientation)\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.title('Tweet Emotion Orientation greater than 30')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vWrnh59QAzSo"
      },
      "outputs": [],
      "source": [
        "orientation_negative_gt50_cl0 = ('NEGATIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 0)\n",
        "orientation_positive_gt50_cl0 = (('POSITIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 0))\n",
        "orientation_neutral_gt50_cl0 = (('NEUTRAL' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 0))\n",
        "orientation_mone_gt50_cl0 = (('mostly_NEGATIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 0))\n",
        "orientation_mopo_gt50_cl0 = (('mostly_POSITIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 0))\n",
        "orientation_na_gt50_cl0 = (('NA' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 0))\n",
        "orientation_negative_gt50_cl1 = ('NEGATIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 1)\n",
        "orientation_positive_gt50_cl1 = (('POSITIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 1))\n",
        "orientation_neutral_gt50_cl1 = (('NEUTRAL' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 1))\n",
        "orientation_mone_gt50_cl1 = (('mostly_NEGATIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 1))\n",
        "orientation_mopo_gt50_cl1 = (('mostly_POSITIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 1))\n",
        "orientation_na_gt50_cl1 = (('NA' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 1))\n",
        "orientation_negative_gt50_cl2 = ('NEGATIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 18)\n",
        "orientation_positive_gt50_cl2 = (('POSITIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 18))\n",
        "orientation_neutral_gt50_cl2 = (('NEUTRAL' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 18))\n",
        "orientation_mone_gt50_cl2 = (('mostly_NEGATIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 18))\n",
        "orientation_mopo_gt50_cl2 = (('mostly_POSITIVE' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 18))\n",
        "orientation_na_gt50_cl2 = (('NA' == emo_rt[\"orientation\"]) & (emo_rt[\"cluster\"] == 18))\n",
        "\n",
        "objects_orientation = ('Negative', 'Mostly Negative', 'Neutral' ,'Mostly Positive', 'Positive', 'NA')\n",
        "\n",
        "y_pos = np.arange(len(objects_orientation))\n",
        "#num_ori_gt50_cl0 = [num_n_gt50_cl0, num_mn_gt50_cl0, num_nu_gt50_cl0, num_mp_gt50_cl0, num_p_gt50_cl0, num_na_gt50_cl0]\n",
        "performance_ori_gt50_cl0 = [orientation_negative_gt50_cl0.sum(), orientation_mone_gt50_cl0.sum(), orientation_neutral_gt50_cl0.sum(), orientation_mopo_gt50_cl0.sum(), orientation_positive_gt50_cl0.sum(), orientation_na_gt50_cl0.sum()]\n",
        "performance_ori_gt50_cl1 = [orientation_negative_gt50_cl1.sum(), orientation_mone_gt50_cl1.sum(), orientation_neutral_gt50_cl1.sum(), orientation_mopo_gt50_cl1.sum(), orientation_positive_gt50_cl1.sum(), orientation_na_gt50_cl1.sum()]\n",
        "performance_ori_gt50_cl2 = [orientation_negative_gt50_cl2.sum(), orientation_mone_gt50_cl2.sum(), orientation_neutral_gt50_cl2.sum(), orientation_mopo_gt50_cl2.sum(), orientation_positive_gt50_cl2.sum(), orientation_na_gt50_cl2.sum()]\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(y_pos, performance_ori_gt50_cl0, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_orientation)\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.title('Tweet Emotion Orientation greater than 30 [cluster 0]')\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(y_pos, performance_ori_gt50_cl1, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_orientation)\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.title('Tweet Emotion Orientation greater than 30 [cluster 1]')\n",
        "\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(y_pos, performance_ori_gt50_cl2, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_orientation)\n",
        "plt.ylabel('Number of tweets')\n",
        "plt.title('Tweet Emotion Orientation greater than 30 [cluster 2]')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lY3PK5YYPn8j"
      },
      "source": [
        "記事の準備"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VNfIP3XyPui8"
      },
      "outputs": [],
      "source": [
        "https = []\n",
        "for q in range(len(df1)):\n",
        "  if q in elim_http:\n",
        "    l = df1.loc[q,:]\n",
        "    https.append(l)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UJm9Y7jz1nO"
      },
      "outputs": [],
      "source": [
        "https = pd.DataFrame(https, columns = [\"id\", \"user_name\", \"time\", \"text\", \"place\", \"favo\", \"RT\", \"follower\"])\n",
        "https =https.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ISveTxWx15UP"
      },
      "outputs": [],
      "source": [
        "http_list = []\n",
        "for r in range(len(https)):\n",
        "  myString = https[\"text\"][r]\n",
        "  myString_list = [item for item in myString.split(\" \")]\n",
        "  for item in myString_list:\n",
        "    try:\n",
        "      match = re.search(\"(?P<url>https?://[^\\s]+)\", myString)\n",
        "      if match is not None: \n",
        "        urls = match.group(\"url\")\n",
        "        http_list.append(urls)\n",
        "    except:\n",
        "      pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naoikvUO5YKo"
      },
      "outputs": [],
      "source": [
        "http_data = pd.DataFrame(http_list)\n",
        "http_data_dudel = http_data.drop_duplicates(keep = 'last')\n",
        "http_data_dudel = http_data_dudel.reset_index(drop=True)\n",
        "#http_list = set(http_list) #最初に回す場合\n",
        "##一度回してから\n",
        "droped_url = []\n",
        "http_get = []\n",
        "csv_files_http = glob.glob(\"/content/drive/My Drive/Tweet/headline/*.csv\")\n",
        "for headfile in csv_files_http:\n",
        "  http_get.append(pd.read_csv(headfile, lineterminator='\\n'))\n",
        "http_get = pd.concat(http_get, axis=0, sort=False)\n",
        "http_get = http_get.drop_duplicates()\n",
        "http_get=http_get.reset_index(drop=True)\n",
        "http_dudel_list = http_data_dudel[0].values.tolist()\n",
        "http_get = http_get[\"URL\"].values.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PHPu3U339xg"
      },
      "outputs": [],
      "source": [
        "for i in range(len(http_dudel_list)):\n",
        "  if http_dudel_list[i] not in http_get:\n",
        "    droped_url.append(http_dudel_list[i])\n",
        "http_list = droped_url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s3Mz8Wi6HyT1"
      },
      "outputs": [],
      "source": [
        "title_list = []\n",
        "with open('/content/drive/My Drive/Tweet/headline/210111_vactine_article_3.csv', 'w',newline='',encoding='utf-8') as f: #change number if runtime error happens\n",
        "  writer = csv.writer(f, lineterminator='\\n')\n",
        "  writer.writerow([\"URL\", \"Headline\"])\n",
        "  for url in http_list:\n",
        "      try:\n",
        "          r = requests.get(url, timeout=3)\n",
        "          soup = BeautifulSoup(r.content, \"html.parser\")\n",
        "          html_element = soup.find(\"h1\")\n",
        "          article_title = html_element.text.strip()\n",
        "          line = [url, article_title]\n",
        "          title_list.append(line)\n",
        "          writer.writerow(line)\n",
        "      except Exception as e:\n",
        "          pass  # ignore any pages where there is a problem"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zz4alyZ0IIYk"
      },
      "outputs": [],
      "source": [
        "#全URL取得後ここから実行\n",
        "http_get = []\n",
        "csv_http = glob.glob(\"/content/drive/My Drive/Tweet/headline/*.csv\")\n",
        "for headfile in csv_http:\n",
        "  http_get.append(pd.read_csv(headfile, lineterminator='\\n'))\n",
        "http_get = pd.concat(http_get, axis=0, sort=False)\n",
        "http_get = http_get.drop_duplicates()\n",
        "http_get=http_get.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drop_java_na = []\n",
        "for i in range(len(http_get)):\n",
        "  if \"JavaScript is not available.\" in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif '404' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'お探しの情報は' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'お探しのページが見つかりませんでした' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'ページが見つかりませんでした' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'Nothing to see here' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif http_get.iloc[i][\"Headline\"] == Null:\n",
        "    drop_java_na.append(i)\n",
        "  elif '403' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'Forbidden' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  #yahoo news, ニコニコ動画, 質問箱などのサイト名がh1にあるケースも削除する\n",
        "  elif 'Yahoo!ニュース' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'ニコニコ動画' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'Peing' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'Not Found' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'FC2動画' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'Bloomberg' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif '国際ニュース：AFPBB News' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'Service Unavailable' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif '調布経済新聞' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif '共同通信' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'SBS 뉴스' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif '福島民友' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)\n",
        "  elif 'ESPN' in str(http_get.iloc[i][\"Headline\"]):\n",
        "    drop_java_na.append(i)"
      ],
      "metadata": {
        "id": "Rsxeu9VVMCvN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "http_available = http_get.drop(drop_java_na)\n",
        "http_available = http_available.dropna()\n",
        "http_available = http_available.reset_index(drop=True)\n",
        "http_available.to_csv(\"/content/drive/My Drive/Tweet/headline/220117_available_test.csv\")"
      ],
      "metadata": {
        "id": "eO3RxzVVNdmC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "頻出単語の分析"
      ],
      "metadata": {
        "id": "nH77lz8tc4Pu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Remove_word(list):\n",
        "  rems = ['@', ',', '，', '。', '', ' ', 'http', 'https','//', '、', '×', '.',\n",
        "            '+', '＋', '/',', ', '(', ')', '（', '）', '://', 'RT', ':', ';','：', '；',\n",
        "            '」','「', '＠', '-', '／',  '%', '％', '!', '！', '#', '＃',\n",
        "            '$', '＄', '=', '＝', 'ー', '￥', '^', '{', '}', '~', '～', \n",
        "            '&', '＆', '・', '?', '??', '？', '？？',  '<', '＜', '>', '＞', '_', '＿', \n",
        "            'co', 'jp', '(@', '○', '『', '』', '”', '’', '\"', '…', '‼', '(+'\n",
        "            ')」', '〇', '【', '】', '[', ']', '⁉', 'gt',\n",
        "            'から', 'より', 'こそ', 'でも', 'しか','さえ', 'けれど', 'たり', 'つつ', 'とも',\n",
        "            'たら', 'ある', 'なら', 'のに', 'です', 'ます', 'する', 'ほど', 'ない', 'くる',\n",
        "            'なり', 'そう', 'まし', 'その', 'この', 'あの', 'せる', 'どう', 'ため', 'どこ',\n",
        "            'いる', 'これ', 'それ', 'あれ', 'いい', 'など', 'あっ', 'もう', 'さん', 'じゃ',\n",
        "            'から', 'あり', 'ので', 'とも', 'ませ', 'でし', 'とき', 'こと', 'なる', 'って',\n",
        "            'ただ', 'まで', 'もの', 'つか', 'なっ', 'でき', 'もっ', 'けど', 'ほぼ', 'なー',\n",
        "            'そこ', 'ここ', 'だろ', 'なん', 'だっ', 'なあ', 'っけ', 'せる', 'やっ', 'また',\n",
        "            'どれ', 'なれ', 'かも', 'いく', 'いけ', 'いう', 'たい', 'あと', 'かも', 'しれ',  \n",
        "            'こう', 'なく', 'よく', 'だけ', 'れる', 'よう', 'かけ', 'どの', 'てる', 'とか',\n",
        "            'という', '思う', '思っ', 'として', 'ところ', 'しまう', 'なんか', 'そんな', \n",
        "            'でしょ', 'しまい', 'わかり', 'まま'\n",
        "            '\\u200d', '\\u3000#'\n",
        "            ]\n",
        "  for number in range(10000):\n",
        "    rems.append(str(number))\n",
        "\n",
        "# 一文字のひらがなとカタカナ，全角数字，アルファベット大文字・小文字\n",
        "  hirakana    = [chr(i) for i in range(12353, 12436)]\n",
        "  katakana    = [chr(i) for i in range(12449, 12533)]\n",
        "  zenkaku_num = [chr(i) for i in range(65296, 65296+10)]\n",
        "  alph_L      = [chr(i) for i in range(97, 97+26)]\n",
        "  alph_S      = [chr(i) for i in range(65, 65+26)]\n",
        "\n",
        "  for a in range(len(hirakana)):\n",
        "    rems.append(hirakana[a])\n",
        "  for b in range(len(katakana)):\n",
        "    rems.append(katakana[b])\n",
        "  for c in range(len(zenkaku_num)):\n",
        "    rems.append(zenkaku_num[c])\n",
        "  for d in range(len(alph_L)):\n",
        "    rems.append(alph_L[d])\n",
        "  for e in range(len(alph_S)):\n",
        "    rems.append(alph_S[e])\n",
        "\n",
        "  words = [x for x in list if (x not in rems)]\n",
        "  return words"
      ],
      "metadata": {
        "id": "SEiooUWTcjWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_word(text):\n",
        "    tokens = t.tokenize(text)\n",
        "    return [token.base_form for token in tokens \n",
        "        if token.part_of_speech.split(',')[0] in['名詞', '動詞','形容詞']]\n",
        "\n",
        "all_users_words=[]\n",
        "\n",
        "for i in range(len(http_available)):\n",
        "  user_text=http_available.iloc[i]['Headline']\n",
        "  #text=list(tagger.parse(user_text).split())\n",
        "  words=extract_word(user_text)\n",
        "  all_users_words.append(words)\n",
        "  \n",
        "all_users_words1=[e for inner_list in all_users_words for e in inner_list]\n",
        "import pickle\n",
        "f = open('/content/drive/My Drive/Tweet/w2b/220117_vaccine.txt', 'wb')\n",
        "list_row = all_users_words1\n",
        "pickle.dump(list_row, f)"
      ],
      "metadata": {
        "id": "lvl3ATt4fpgV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_chain = ' '.join(list_row)\n",
        "stop_words = [u'くる', u'する', u'なる', u'ない', u'こと', u'れる', u'せる', u'of', u'the', u'いる', u'さん', u'in', u'to', u'できる']\n",
        "wordcloud = WordCloud(background_color=\"white\", font_path = '/usr/share/fonts/truetype/fonts-japanese-mincho.ttf', collocations = False, stopwords = set(stop_words), width=600,height=400,min_font_size=15)\n",
        "wordcloud.generate(word_chain)\n",
        "wordcloud.to_file(\"/content/drive/My Drive/Tweet/figures/220117_article_wordcloud.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0YyqPd3gh4E",
        "outputId": "0739c19c-9641-4bdf-9975-e604a3ec5663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wordcloud.wordcloud.WordCloud at 0x7ffad4c55d10>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "記事の感情分析"
      ],
      "metadata": {
        "id": "yx1pOSGEqsLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_article_list = []\n",
        "orientation_article_list = []\n",
        "\n",
        "for k in range(len(http_available)):\n",
        "  headline = http_available.iloc[k]['Headline']\n",
        "  result_headline = emotion_analyzer.analyze(headline)\n",
        "  try:\n",
        "    orientation_headline = result_headline['orientation']\n",
        "    kanjo_headline = result_headline['representative']\n",
        "    emotion_article_list.append(kanjo_headline)\n",
        "    orientation_article_list.append(orientation_headline)\n",
        "  except KeyError:\n",
        "    kanjo_headline = 'NA'\n",
        "    orientation_headline = 'NA'\n",
        "    emotion_article_list.append(kanjo_headline)\n",
        "    orientation_article_list.append(orientation_headline)"
      ],
      "metadata": {
        "id": "JfhMOtk6qy02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "headline_emotion = http_available\n",
        "headline_emotion[\"orientation\"] = orientation_article_list\n",
        "headline_emotion[\"emotion\"] = emotion_article_list"
      ],
      "metadata": {
        "id": "xy00oVUIs2zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "orientation_article_negative = (headline_emotion['orientation'] == 'NEGATIVE')\n",
        "orientation_article_positive = (headline_emotion['orientation'] == 'POSITIVE')\n",
        "orientation_article_neutral = (headline_emotion['orientation'] == 'NEUTRAL')\n",
        "orientation_article_mone = (headline_emotion['orientation'] == 'mostly_NEGATIVE')\n",
        "orientation_article_mopo = (headline_emotion['orientation'] == 'mostly_POSITIVE')\n",
        "orientation_article_na = (headline_emotion['orientation'] == 'NA')\n",
        "#RTやhttpを除くツイートの感情(positive vs. negative)の図示\n",
        "objects_orientation = ('Negative', 'Mostly Negative', 'Neutral' ,'Mostly Positive', 'Positive', 'NA')\n",
        "import matplotlib.pyplot as plt\n",
        "y_pos = np.arange(len(objects_orientation))\n",
        "performance = [orientation_article_negative.sum(), orientation_article_mone.sum(), orientation_article_neutral.sum(), orientation_article_mopo.sum(), orientation_article_positive.sum(), orientation_article_na.sum()]\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.bar(y_pos, performance, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_orientation)\n",
        "plt.ylabel('Number of article')\n",
        "plt.title('Article Emotion Orientation')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3JRp0owQurSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_article_yorokobi = [s for s in headline_emotion['emotion'] if 'yorokobi' in s]\n",
        "emotion_article_ikari = [s for s in headline_emotion['emotion'] if 'ikari' in s]\n",
        "emotion_article_aware = [s for s in headline_emotion['emotion'] if 'aware' in s]\n",
        "emotion_article_kowagari = [s for s in headline_emotion['emotion'] if 'kowa' in s]\n",
        "emotion_article_haji = [s for s in headline_emotion['emotion'] if 'haji' in s]\n",
        "emotion_article_suki = [s for s in headline_emotion['emotion'] if 'suki' in s]\n",
        "emotion_article_iya = [s for s in headline_emotion['emotion'] if 'iya' in s]\n",
        "emotion_article_takaburi = [s for s in headline_emotion['emotion'] if 'takaburi' in s]\n",
        "emotion_article_yasuragi = [s for s in headline_emotion['emotion'] if 'yasu' in s]\n",
        "emotion_article_odoroki = [s for s in headline_emotion['emotion'] if 'odoroki' in s]\n",
        "emotion_article_na = [s for s in headline_emotion['emotion'] if 'NA' in s]\n",
        "objects_emotion = ('joy', 'anger', 'sorrow', 'fear', 'shame', 'liking', 'dislike', 'excitement', 'relief', 'surprise', 'NA')\n",
        "y_pos = np.arange(len(objects_emotion))\n",
        "performance = [len(emotion_article_yorokobi), len(emotion_article_ikari), len(emotion_article_aware), len(emotion_article_kowagari), len(emotion_article_haji), len(emotion_article_suki), len(emotion_article_iya), len(emotion_article_takaburi), len(emotion_article_yasuragi), len(emotion_article_odoroki), len(emotion_article_na)]\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.bar(y_pos, performance, align='center', alpha=0.5, width=0.6)\n",
        "plt.xticks(y_pos, objects_emotion)\n",
        "plt.ylabel('Number of article')\n",
        "plt.title('Article Emotion')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2z6mc6mVvwSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "emotion.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMOQPmyqnMtEqtanuae6hlj",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}